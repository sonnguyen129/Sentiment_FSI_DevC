{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stopwords.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1WwukSj3wbQoha3r_xgUy5hHHzsXsPmlN","authorship_tag":"ABX9TyOh8UOxaRMP+duyIIe/9ENz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Y0l-UCGNpMR4","executionInfo":{"status":"ok","timestamp":1606766036127,"user_tz":-420,"elapsed":1051,"user":{"displayName":"Nguyen Son","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH-z6tfzLe-X7b5hlyOR456VU6LtaImg0P36kV=s64","userId":"12638499436909749549"}}},"source":["import os\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuFF3TKrrzfw","executionInfo":{"status":"ok","timestamp":1606766036128,"user_tz":-420,"elapsed":1044,"user":{"displayName":"Nguyen Son","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH-z6tfzLe-X7b5hlyOR456VU6LtaImg0P36kV=s64","userId":"12638499436909749549"}},"outputId":"72986192-3c9f-494a-d6e0-4e701657a637"},"source":["%cd '/content/drive/MyDrive/Project_DevC'\n","!ls"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Project_DevC\n"," clone_cmt_tiki\t\t    Data_Preprocessing.ipynb\n"," clone_data\t\t    model.h5\n"," Data_Bank100518.txt\t    scrapingweb_bookingcomment.ipynb\n"," Data_ChungKhoan14518.txt  'Script - SơnNT.docx'\n"," Data_chungkhoan_duc.csv    Sentiment_Analysis.ipynb\n"," data_dep.csv\t\t    Sentiment_LSTM.ipynb\n"," datadepvail.csv\t    Untitled10.ipynb\n"," data.pkl\t\t   'Xử lí dữ liệu + LSTM.pptx'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PIgLr8vmq5A3","executionInfo":{"status":"ok","timestamp":1606766036132,"user_tz":-420,"elapsed":1042,"user":{"displayName":"Nguyen Son","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH-z6tfzLe-X7b5hlyOR456VU6LtaImg0P36kV=s64","userId":"12638499436909749549"}}},"source":["def load_data(txt_file):\n","    \"\"\"\n","    Each document is one line, documents is already preprocess like: remove truncate, tokenize, strip, ...\n","    :param txt_file: path/to/text/file\n","    :return: list of documents\n","    \"\"\"\n","    texts = []\n","    with open(txt_file, 'r', encoding='utf8') as fp:\n","        for line in fp.readlines():\n","            texts.append(line.strip())\n","    return texts"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"i1cXa4aErFNe","executionInfo":{"status":"ok","timestamp":1606766036925,"user_tz":-420,"elapsed":1829,"user":{"displayName":"Nguyen Son","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH-z6tfzLe-X7b5hlyOR456VU6LtaImg0P36kV=s64","userId":"12638499436909749549"}}},"source":["def get_stopwords(documents, threshold=2):\n","    \"\"\"\n","    :param documents: list of documents\n","    :param threshold:\n","    :return: list of words has idf <= threshold\n","    \"\"\"\n","    tfidf = TfidfVectorizer(min_df=100)\n","    tfidf_matrix = tfidf.fit_transform(documents)\n","    features = tfidf.get_feature_names()\n","    stopwords = []\n","    print(min(tfidf.idf_), max(tfidf.idf_), len(features))\n","    for index, feature in enumerate(features):\n","        if tfidf.idf_[index] <= threshold:\n","            stopwords.append(feature)\n","    return stopwords"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qe3CG6vYrJWw","executionInfo":{"status":"ok","timestamp":1606766099207,"user_tz":-420,"elapsed":1054,"user":{"displayName":"Nguyen Son","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH-z6tfzLe-X7b5hlyOR456VU6LtaImg0P36kV=s64","userId":"12638499436909749549"}},"outputId":"6b0ae913-cd1d-45cc-8766-dac0a0cc5749"},"source":["if __name__ == '__main__':\n","    docs = load_data(r\"/content/drive/MyDrive/Project_DevC/Data_ChungKhoan14518.txt\")\n","    stopwords = get_stopwords(docs, threshold=2)\n","    with open('stopwords.txt', 'w', encoding='utf8') as fp:\n","        for word in stopwords:\n","            fp.write(word + '\\n')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["1.4254324936220328 4.990632851554459 339\n"],"name":"stdout"}]}]}